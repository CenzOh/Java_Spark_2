//Lesson 53
package com.virtualpairprogrammers;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Scanner;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.Optional;
import org.spark_project.guava.collect.Iterables;

import scala.Tuple2;

public class Lesson53 {
	
	public static void main(String[] args) {

	
		//Lesson 53 Dealing with Key Screws
/* 
 * 
 */
	
		System.setProperty("hadoop.home.dir", "c:/hadoop"); 
		Logger.getLogger("org.apache").setLevel(Level.WARN); 
		
		SparkConf conf = new SparkConf().setAppName("startingSpark").setMaster("local[*]"); 
		JavaSparkContext sc = new JavaSparkContext(conf);
		
	//Q: How will initialRdd be partitioned?
	//A: 64mb blocks (at time of recording) so about 6 partitions
		JavaRDD<String> initialRdd = sc.textFile("src/main/resources/bigLog.txt");	
	//key: log level
	//value: date
		System.out.println("Initial RDD Partition SIze: " + initialRdd.getNumPartitions()); 
		
		JavaPairRDD<String, String> warningsAgainstDate = initialRdd.mapToPair( inputLine -> {
			String[] cols = inputLine.split(":");
			String level = cols[0] + (int)(Math.random() * 11); //this is grabbing a random number and adding it to the end of our level such as `WARN6`.
			String date = cols[1];
			return new Tuple2<>(level, date);
		});
		
		System.out.println("After narrow transformation we have " + warningsAgainstDate.getNumPartitions() + " parts");
		
		//Now we are going to do a "wide" transformation
		JavaPairRDD<String, Iterable<String>> results = warningsAgainstDate.groupByKey(); //expensive operation with lots of shuffling
		System.out.println(results.getNumPartitions() + " partitions after the wide transformation");
		
		results.foreach(it -> System.out.println("key " + it._1 + " has " + Iterables.size(it._2)+ " elements"));
/*our output now displays all the levels and we can see how they are spread evenly
 *  key ERROR7 has 445719 elements
 *  key WARN0 has 454415 elements
 *  
 *  Instead of ONE key for each level we have MULTIPLE keys for each level
 *  on spark UI, if we look at stage 1, it was all very skewed but this time, duration min is 0.7 and max is 7s. We can also see tasks are pretty balanced. 2s, 4s, 0,7s, 7s. All 
 *  11 partitions read at least some of the data. Their shuffle read size 3.6 mb, 8.4 mb, 3.7 mb,, etc.
 */
		
		Scanner scanner = new Scanner(System.in);
		scanner.nextLine();
		

		sc.close();
	
	}
	
}
