//Lesson 54
package com.virtualpairprogrammers;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Scanner;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.Optional;
import org.spark_project.guava.collect.Iterables;

import scala.Tuple2;

public class Lesson54 {
	
	public static void main(String[] args) {

	
		//Lesson 54 Caching and Persistence
/* We will demonstrate this with a simple example, the partition test. Terminate the program and lets update the code below: Edit the line with String level = cols[0]
 * So note that we dont have the salting anymore, avoid it unless necessary in real life! So we are running through our transformation and counting how many elements for each key.
 * What if we want to get a grand total? See below
 */
	
		System.setProperty("hadoop.home.dir", "c:/hadoop"); 
		Logger.getLogger("org.apache").setLevel(Level.WARN); 
		
		SparkConf conf = new SparkConf().setAppName("startingSpark").setMaster("local[*]"); 
		JavaSparkContext sc = new JavaSparkContext(conf);
		
	//Q: How will initialRdd be partitioned?
	//A: 64mb blocks (at time of recording) so about 6 partitions
		JavaRDD<String> initialRdd = sc.textFile("src/main/resources/bigLog.txt");	
	//key: log level
	//value: date
		System.out.println("Initial RDD Partition SIze: " + initialRdd.getNumPartitions()); 
		
		JavaPairRDD<String, String> warningsAgainstDate = initialRdd.mapToPair( inputLine -> {
			String[] cols = inputLine.split(":");
			String level = cols[0];  //this line to remove the salting
			String date = cols[1];
			return new Tuple2<>(level, date);
		});
		
		System.out.println("After narrow transformation we have " + warningsAgainstDate.getNumPartitions() + " parts");
		
		//Now we are going to do a "wide" transformation
		JavaPairRDD<String, Iterable<String>> results = warningsAgainstDate.groupByKey(); 
		System.out.println(results.getNumPartitions() + " partitions after the wide transformation");
		
		results.foreach(it -> System.out.println("key " + it._1 + " has " + Iterables.size(it._2)+ " elements"));

		System.out.println(results.count()); //simple print out total! HOWEVER, big problem here. Serious performance issue
/* Very subtle to spot. Run the program and look at web ui. But first for our output we get"
 * key ERROR has 5001114 elements
 * key WARN has 4998886 elements 
 * 
 * so we have the two stages as usual and it outputs elements for the keys which is what we wanted. HOWEVER, when we call count it runs ANOTHER stage. So this suggests when we do count
 * we had to do a shuffle? Why? Well lets look at the spark UI. We see a list of all the actions that were performed. So we see:
 * Job ID | Desc   | Stages   | Tasks
 * 1	  | count  | 1/1	  | 11/11 Both stages and tasks here show `1 skipped` and `11 skipped`
 * 0	  | foreach| 2/2	  | 22/22
 * 
 * We only had ONE entry which was the for each. But now we have a separate entry for count. Foreach is the same as before. But in the count we can see this is the stage 3 we did not
 * expect. It has done a groupByKey AGAIN. It also has a stage 2 which is skipped. WE'll come back to that. But why is it running another stage? Looks like it is redoing
 * previous steps that were already done. If we look at the first job (ID 0) we can see it already did groupByKey and mapValues. SOmething wrong here.
 * 
 * SO this comes back to the fact that when we have a Java RDD we dont really have any data at all. Again, no data is loaded in the JavaRDD<String> initialRDD. We are again, adding elements
 * to the execution plan. It is only when it reaches an action such as foreach that it becomes a job and executes the plan. The foreach is our first action. So it will do the
 * transformations, shuffling, grouping, then output the results. Once it outputs the results, the real RDD in memory / actual data will be discarded. So now, when it comes to the counts,
 * it has to go all the way back to execute those steps AGAIN.
 */
		
		Scanner scanner = new Scanner(System.in);
		scanner.nextLine();
		

		sc.close();
	
	}
	
}
