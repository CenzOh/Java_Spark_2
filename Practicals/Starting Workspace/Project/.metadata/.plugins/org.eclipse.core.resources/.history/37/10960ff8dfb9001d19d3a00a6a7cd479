package com.virtualpairprogrammers;

import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.Comparator;
import java.util.Date;
import java.util.List;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

import scala.Tuple2;

public class Lesson124 {

/* Lesson 124 Starting a streaming job. 
 * 
 * We will write a streaming job to constantly take in this stream of messages. Spark streaming is NOT a part of spark core so we have to add a dependency into our 
 * pom.xml. Add the following to match with our ver of spark core:
 * 
 *  <dependency>
	      <groupId>org.apache.spark</groupId>
	      <artifactId>spark-streaming_2.11</artifactId>
	      <version>2.3.2</version>
    </dependency>
 *
 * Again the 2.11 is the scala version. Make sure we have same 2.3.2 ver as spark core. Then run pom.xml as a maven build. GOal eclipse:eclipse and eclipse:clean.
 * What this does is we tell eclipse to download all the jars and have update the local build path inside eclipse. Find all jar files, download. Once it says build success go to the project
 * right click, refresh. Jar files should ahve spark-streaming_2.11 jar files.
 * 
 * NOTE TO SELF after adding this and following the instructions my project was unable to compile a lot of things it says issue is something like source level 1.5 or higher? Not sure
 * going to finish the module as is anyway.
 * 
 * One of features of spark streaming is similar to how we work in regular spark when doing batch jobs. Main difference is setup is a bit different. 
 * First part with setting up conf object is same as before! Java string context is a bit differnt. We can compare spark streaming example code with our LogCountRDDVersion.java.
 * We first set up the entry point for the spark conf object. Theres an interesting parameter in java streaming conetxt, Durations. THis tells us how big do we want the
 * batch to be? Example says .seconds(1). This means there will be a new build every second! 
 * 
 * Lets save time and reuse some code from log count rdd version .java file. We will make a new class called LogStreamAnalysis
 */
	@SuppressWarnings("resource") 
	public static void main(String[] args) {
		System.setProperty("hadoop.home.dir", "c:/hadoop"); 
		Logger.getLogger("org.apache").setLevel(Level.WARN); 
	
		SparkConf conf = new SparkConf().setMaster("local[*]").setAppName("startingSpark");
		JavaSparkContext sc = new JavaSparkContext(conf);
		JavaRDD<String> input = sc.textFile("Src/main/resources/biglog.txt"); 
		
		// remove the csv header
		input = input.filter(line -> !line.startsWith("level,datetime"));
		
		JavaPairRDD<String, Long> pairs = input.mapToPair(rawValue -> {
			String[] csvFields = rawValue.split(",");
			String level = csvFields[0];
			String date = csvFields[1];
			String month = rawDateToMonth(date); 
			String key = level + ":" + month; 
			return new Tuple2<>(key, 1L); 
		});
		
		JavaPairRDD<String, Long> resultsRdd = pairs.reduceByKey((value1, value2) -> value1 + value2);
		
		// order by 
		Comparator<String> comparator = serialize( (a,b) -> {
			String monthA = a.split(":")[1];
			String monthB = b.split(":")[1];
			return monthToMonthnum(monthA) - monthToMonthnum(monthB);
		});
		
		//assuming it is a stable sort, we can sort by secondary first (level) and then sort by primary (month).
		resultsRdd = resultsRdd.sortByKey().sortByKey(comparator);
		
		List<Tuple2<String, Long>> results = resultsRdd.take(100);
		
		System.out.println("Level\tMonth\t\tTotal");
		for (Tuple2<String, Long> nextResult : results) {
			String[] levelMonth = nextResult._1.split(":");
			String level = levelMonth[0];
			String month = levelMonth[1];
			Long total = nextResult._2;
			System.out.println(level+"\t" + month + "\t\t" + total);
		}
	}
	
	private static String rawDateToMonth(String raw) { 
		SimpleDateFormat rawFmt = new SimpleDateFormat("yyyy-M-d hh:mm:ss"); 
		SimpleDateFormat requiredFmt = new SimpleDateFormat("MMMM"); 
		Date results;
		try {
			results = rawFmt.parse(raw); 
			String month = requiredFmt.format(results); 
			return month;
		}
		
		catch (ParseException e) {
			throw new RuntimeException(e);
		}
	}
	
	private static int monthToMonthnum(String month) {
		SimpleDateFormat rawFmt = new SimpleDateFormat("MMMM");
		SimpleDateFormat requiredFmt = new SimpleDateFormat("M");
		Date results;
		
		try {
			results = rawFmt.parse(month);
			int monthNum = new Integer(requiredFmt.format(results));
			return monthNum;
		}
		
		catch (ParseException e) {
			throw new RuntimeException(e);
		}
	}
}
