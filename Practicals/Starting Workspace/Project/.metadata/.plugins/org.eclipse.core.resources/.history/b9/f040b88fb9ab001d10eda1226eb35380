package com.virtualpairprogrammers;

import java.util.Scanner;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;

import static org.apache.spark.sql.functions.*;


public class Lesson81 {
	
/* Lesson 81 How does HashAggregation work? 
 * TO learn about hash aggregation, we can check out the spark sql documentation. Its great and fast. However, at time of recording, there are not any real good information
 * when searching for hash aggregation. We get results for the java dpc but we want to see it in apache spark. For the java one, we found a page for SparkStrategies.HashAggregation
 * however, there is no detail in here. Its just a bad time of recording.
 * 
 * What if we come from a database background, being familiar with difference between hash aggregation and group aggregation? It is used by some databases.
 * If we search for this, we can see PostgreSQL shows up a lot and uses a similar strategy. Interestingly, reading the PostgreSQL reference document will tell us a lot more.
 * 
 * We will receive brief overview about these two algorithms and why do we see one for the java api search and the other is slower for SQL search. So, we have seen that Saprk SQL
 * can use two algorithms for grouping which is used for doing groupBy. Again, these are SortAggregate and HashAggregate.
 * 
 * SortAggregate, this groups by sorting the rows using a regular source algorithm and then it gathers together the matching rows. This is pretty much how we did it earlier
 * in the course when we grouped together the values for that row. Before the group, you get them in order in the first place to gather together the rows with the same level.
 * 
 * Level, Group of DateTimes
 * WARN, [2016-12-31 04:19:32, 2016-12-31 ...]
 * FATAL, [2016-12-31 03:22:34, 2015-4-21...]
 * 
 * BIG DOWNSIDE of this sort aggregation algorithm is that it has to perform a sort. They are not a slow solution but as the input set for the sort grows, the time it takes to do sorts
 * grows more than linearly. Performance: O(n*log n) but memory efficient. So if input data size is doubled, then the sort will take LONGER than TWICE as long.
 * Since we are dealing with massive datasets across partitions, there will be a serious penalty from doing sorts. In favor of sorts, they can be memory efficient. They are done in
 * place meaning if we have an input data set, we can really reorder the rows and we dont particularly need lots of extra memory to implement it. Depends on sorting implementation
 * but generally sorts are more efficient but performance will degrade as input set grows.
 * 
 *  There is an alternative algorithm called hash aggregations. Lets review the SQL statement 
 */
	
	@SuppressWarnings("resource") 
	public static void main(String[] args) {
		System.setProperty("hadoop.home.dir", "c:/hadoop");
		Logger.getLogger("org.apache").setLevel(Level.WARN);
	
		SparkSession spark = SparkSession.builder().appName("testingSql").master("local[*]")
				.config("spark.sql.warehouse.dir", "file:///c:/tmp/")
				.getOrCreate();
		
		spark.conf().set("spark.sql.shuffle.partitions", "12");

				
		Dataset<Row> dataset = spark.read().option("header", true).csv("Src/main/resources/biglog.txt");
		
		
// SQL reference
		dataset.createOrReplaceTempView("logging_table"); 

		Dataset<Row> results = spark.sql
			("select level, date_format(datetime, 'MMMM') as month, count(1) as total "
			+ "from logging_table group by level, month order by cast(first(date_format(datetime, 'M')) as int), level"); 
	
		results.show(100);
		
		results.explain();


			
// Java API / DataFrame version		
//		dataset = dataset.select(col("level"), 
//						  date_format(col("datetime"), "MMMM").alias("month"), 
//					  	  date_format(col("datetime"), "M").alias("monthnum").cast(DataTypes.IntegerType) );
//	
//		
//		dataset = dataset.groupBy("level", "month", "monthnum").count().as("total").orderBy("monthnum");
//		dataset = dataset.drop("monthnum");
// 
//		dataset.show(100);
//		
//		dataset.explain();

		
		Scanner scanner = new Scanner(System.in); 
		scanner.hasNextLine();
		

		spark.close();
	
	}
}
