//Leson 26
package com.virtualpairprogrammers;

import java.util.Arrays;
import java.util.List;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

import scala.Tuple2;

public class Lesson25 {
	
	public static void main(String[] args) {

	
		//Lesson 26 why coalesce is the WRONG solution
/* Last time we used coalesce to force spark to move all our data onto ONE partition. Appeared to solve our problem. HOWEVER, it did not. Many people online will say this is the answer
 * Lets explain why! What we do with coalesce is actually creating a new problem!! Lets assume after doing this, we want to do some further operation on the side.
 * And assume that we are still in realm of big data. So by combining it all onto a single partition, it will be on a single node and single physical JVM. We may end up with an out of memory
 * exception. Will not get any further benefits from spark. This is the wrong solution because we have the wrong explanation. We need a better understanding. 
 * Lets go over the CORRECT explanation. 
 * 
 */
	
		System.setProperty("hadoop.home.dir", "c:/hadoop"); 
		Logger.getLogger("org.apache").setLevel(Level.WARN); 
		
		SparkConf conf = new SparkConf().setAppName("startingSpark").setMaster("local[*]"); 
		JavaSparkContext sc = new JavaSparkContext(conf);
	
		JavaRDD<String> initialRdd = sc.textFile("src/main/resources/subtitles/input.txt");	
		
		JavaRDD<String> lettersOnlyRdd = initialRdd.map( sentence -> sentence.replaceAll(" ^[a-zA-Z\\s] ", "").toLowerCase()); 
		
		JavaRDD<String> removedBlankLines = lettersOnlyRdd.filter( sentence -> sentence.trim().length() > 0); 
	
		JavaRDD<String> justWords = removedBlankLines.flatMap( sentence -> Arrays.asList(sentence.split(" ")).iterator());
		
		JavaRDD<String> blankWordsRemoved = justWords.filter(word -> word.trim().length() > 0);
	
		JavaRDD<String> justInterestingWords = blankWordsRemoved.filter( word -> Util.isNotBoring(word));
		
		JavaPairRDD<String, Long> pairRdd = justInterestingWords.mapToPair( word -> new Tuple2<String, Long>(word, 1L));
	
		JavaPairRDD<String, Long> totals = pairRdd.reduceByKey((value1, value2) -> value1 + value2);

		JavaPairRDD<Long, String> switched = totals.mapToPair(tuple -> new Tuple2<Long, String> (tuple._2, tuple._1 ));

		JavaPairRDD<Long, String> sorted = switched.sortByKey(false);
		
//We can prove this was the WRONG explanation. There is a method on our RDD called get num of partitions. Will tell us how many partitions there are. We never configged spark to use a certain amount
		System.out.println("There are " + sorted.getNumPartitions() + " partitions"); //prints 2 partitions

		
		sorted = sorted.coalesce(1); //WRONG ANSWER BUT FOR TESTING PURPOSES. Output will seem like it works and sorts correctly. 

		sorted.foreach(System.out::println);

		
//		List<Tuple2<Long, String>> results = sorted.take(50);
//		results.forEach(System.out::println);
				
		sc.close();
	
	}

}
