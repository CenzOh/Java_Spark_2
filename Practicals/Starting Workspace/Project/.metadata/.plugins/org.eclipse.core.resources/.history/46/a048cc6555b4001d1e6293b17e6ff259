package com.virtualpairprogrammers;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.ml.feature.VectorAssembler;
import org.apache.spark.ml.param.ParamMap;
import org.apache.spark.ml.regression.LinearRegression;
import org.apache.spark.ml.tuning.ParamGridBuilder;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class Lesson100 {

/* Lesson 100 Training Testing and Holdout data
 * 
 * Recall when we will split our data, 80% will be training and 20% will be for testing / evaluating. Now we will ask Spark to run the model building process tons of times to see which is
 * the best set of parameters. So we will split our data into three sections. Largest part to build the model, Spark uses next part to evaluate which is the best one, finally we will
 * use the third set of data to do our own evaluation to see if Spark produced a good model or not. Correct terminology is Training data, Testing data, Hold out data. 
 * 
 * SPark uses the Test data when it creates lots of models to find the best fitting model. We will use holdout data to decide if the model is good or not.
 * Recall when we originally did 80-20 split we created one data set for training and testing, and a second data set for hold out. Now we will split that first dataset into two
 * separate data set. Spark will do that for us. We have to create a train validation split. This object requires us to set four values:
 * First is the estimator, this is the model builder we are using (linear regression).
 * Second is evaluator, the instruction to say how we will determine best model from the result (such as maximize r squared, minimize the root mean squared error).
 * paramMap, which we just created.
 * Finally, trainRatio which is what proportion of the data are we supplying should be used for training? We can do 0.8, 80 percent training, 20 percent testing.
 * 
 * Why do we need the hold out data? Why cant spark do the job with just testing and training? Its because test data is being used every single time, it will become of the model building process.
 * Spark will give us the best model that fits the test data. Might not be the best model we can get so we need to do an independent verification that what Spark produces for us is
 * a good model. SO we reserve / hold back some data to use to do that evaluation. THis data is not used in model building process at all, completely independent for a fair view that
 * what Spark produced is good. 
 * 
 * If the model fits test data but not hold out data this is known as OVERFITTING - We improved the model so much that it works for test data but not for general datasets. AVOID THIS with holdout
 * data. 
 */
	
	public static void main(String[] args) {
		
		System.setProperty("hadoop.home.dir", "c:/hadooop"); 
		Logger.getLogger("org.apache").setLevel(Level.WARN);
		
		SparkSession spark = SparkSession.builder()
				.appName("Gym Competitors")
				.config("spark.sql.warehouse.dir", "file:///c:/tmp/")
				.master("local[*]")
				.getOrCreate();
		
		Dataset<Row> csvData = spark.read()
				.option("header",  true)
				.option("inferSchema", true)
				.csv("src/main/resources/kc_house_data.csv"); 

		VectorAssembler vectorAssembler = new VectorAssembler()
				.setInputCols(new String[] {"bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "grade"} )
				.setOutputCol("features");
		
		Dataset<Row> modelInputData = vectorAssembler.transform(csvData)
				.select("price", "features")
				.withColumnRenamed("price",  "label");
		
//		modelInputData.show();
		
		Dataset<Row>[] trainingAndTestData = modelInputData.randomSplit(new double[] {0.8, 0.2});
		Dataset<Row> trainingAndTestDataData = trainingAndTestData[0]; //renamed from traingData to trainingAndTestData
		Dataset<Row> holdOuttData = trainingAndTestData[1];  //renamed from testData to holdOutData
		
		LinearRegression linearRegression = new LinearRegression();
		
		ParamGridBuilder paramGridBuilder = new ParamGridBuilder();

		ParamMap[] paramMap = paramGridBuilder.addGrid(linearRegression.regParam(), new double[] {0.01, 0.1, 0.5})
				.addGrid(linearRegression.elasticNetParam(), new double[] {0, 0.5, 1})
				.build();
		
		System.out.println("Training data r2 value is " + model.summary().r2() + " and the RMSE is " + model.summary().rootMeanSquaredError()); 
		
		model.transform(testData).show(); 
		
		System.out.println("Testing data r2 value is " + model.evaluate(testData).r2() + " and the RMSE is " + model.evaluate(testData).rootMeanSquaredError()); 
			

	}
}