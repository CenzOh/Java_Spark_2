package com.virtualpairprogrammers;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.SparkConf;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;

public class Lesson125 {

/* Lesson 125 Stream Transformation
 * 
 * Lets show how we can show how the code in spark batch mode is similar to streaming and how easy it is
 */
	
	@SuppressWarnings("resource") 
	public static void main(String[] args) throws InterruptedException {
		System.setProperty("hadoop.home.dir", "c:/hadoop"); 
		Logger.getLogger("org.apache").setLevel(Level.WARN); 
	
		SparkConf conf = new SparkConf().setMaster("local[*]").setAppName("startingSpark");
//boilerplate ^^^^^	

		JavaStreamingContext sc = new JavaStreamingContext(conf, Durations.seconds(30));//sc for spark context usually called jsc but thats kinda ugly

		JavaReceiverInputDStream<String> inputData = sc.socketTextStream("localhost", 8989);
		
/* Start here, the inputData has methods from Spark RDD we are familiar with like map, filtering, random, flatMap, flatMapToPairs, reduce function. Our analysis will be a acount of fatals,
 * WARNS, and FATALS. lets see it run first with a basic map just looking at batches of data.
 * We need to do some operation to get a DStream. Lets do a map operation. FOr each item we recieve, map it to an instance of that item. We will end up with a result of type JavaDStream.
 * This is direct equivalent of java RDD. This is NOT an RDD but an equivalent.
 */
		JavaDStream<Object> results = inputData.map(item -> item);
		results.print(); 
		
/* We'll be recieving thousands of messages into this socket so we'll get a ton of results. Currently will not work, spark runs through then exists. Spark streaming is running in effectively,
 * an infinite loop. Recall we have to call sc.start() to START the process, then we need to do sc.awaitTermination() to hold our program open until we terminate JVM
 * We have a compiler expection saying interuptted exception. We are NOT expecting an interuption exception to occur so lets just throw a interruption exception. Not good practice but will do 
 * for now to get spark streaming up and running.
 * Note in prod systems we expect to leave these jobs running forever.
 */
		sc.start();
		sc.awaitTermination();
		
/* Now we can verify the logging server is running. It wont output anything if we see stop button red box lit up then it is running. Note if we make changes to spark job, we must
 * stop and restart logging server because it only takes one connection. It has no way of knowing connection has dropped. 
 * Okay now lets run the log stream analysis program. We can see so many warnings appear, comes from block manager and random block replication policy. THis happens because we are NOT
 * running in a cluster. Super annoying so lets terminate the run.
 * One thing you will notice is at the end of each batch, results are presented to us as a timestamp.
 * 
 * -------------------------------------------
Time: 1677861690000 ms
-------------------------------------------
DEBUG,Fri Mar 03 11:40:59 EST 2023
WARN,Fri Mar 03 11:40:59 EST 2023
INFO,Fri Mar 03 11:40:59 EST 2023
INFO,Fri Mar 03 11:40:59 EST 2023
INFO,Fri Mar 03 11:40:59 EST 2023
DEBUG,Fri Mar 03 11:40:59 EST 2023
INFO,Fri Mar 03 11:40:59 EST 2023
DEBUG,Fri Mar 03 11:40:59 EST 2023
INFO,Fri Mar 03 11:40:59 EST 2023
INFO,Fri Mar 03 11:40:59 EST 2023
...
 */
		
		
		
		
		
	}
}