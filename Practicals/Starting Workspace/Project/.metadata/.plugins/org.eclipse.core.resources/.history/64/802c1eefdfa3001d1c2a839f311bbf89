package com.virtualpairprogrammers;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.sql.Column;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.functions;

import static org.apache.spark.sql.functions.*; //use this instead for .col()

public class Lesson62 {
	
/* Lesson 62 Using a Spark Temporary View for SQL
 * We will be looking at FULL SQL syntax in spark SQL. We can use a java API (will look at later). The SQL API is better if we want to focus on the data science rather than coding.
 * We looked at writing filters, there are several approaches to writing filters. Two java-ish ways, either domain specific language (pretty readable but complicated) or lambdas
 * (not as readable). More common way is to use SQL syntax. WE will look at it in more detail. 
 */
	
	@SuppressWarnings("resource") 
	public static void main(String[] args) {
		System.setProperty("hadoop.home.dir", "c:/hadoop");
		Logger.getLogger("org.apache").setLevel(Level.WARN);
	
		SparkSession spark = SparkSession.builder().appName("testingSql").master("local[*]")
				.config("spark.sql.warehouse.dir", "file:///c:/tmp/")
				.getOrCreate();
		
		Dataset<Row> dataset = spark.read().option("header",  true).csv("src/main/resources/exams/students.csv");

// sql
//		Dataset<Row> modernArtResults2 = dataset.filter("subject = 'Modern Art' AND year >= 2007");
		
// lambda
//		Dataset<Row> modernArtResults = dataset.filter( row -> row.getAs("subject").equals("Modern Art")
//															&& Integer.parseInt(row.getAs("year")) >= 2007);

// functions class
//		Dataset<Row> modernArtResults1 = dataset.filter(col("subject").equalTo( "Modern Art" )
//				  													  .and(col("year").geq(2007)) );
		
		dataset.createOrReplaceTempView("my_students_table");
		
/* If we call this view students, we can refer to that view in a later SQL statement. We can call it whatever we like! In spark can think of a view like a table in a relational databse.
 * Note this method does not return anything. No reference to the view idea. This is because NOW back to the original spark session object there is a method called .sql() it allows us to
 * execute any arbitrary SQL statements against any of the views created. We can do anything we want, but first we will do a filter
 */
		Dataset<Row> results = spark.sql("select * from my_students_table where subject = 'French' "); //returns a dataset of rows
		results.show();

/* student_id | Exam_center_id | subject | year | quarter | Score | grade
 * 			3 | 			 1 |  French | 2005 | 		 1 |    53 |     C
 * 			4 | 			 1 |  French | 2005 | 		 1 |    21 |     E
 * 
 * It works although it is similar to what we have done previously, using filters. Point is we have full expressiveness of SQL available to us. If we are interested in one of the columns
 * we can easily do projecting which just selects a subset of the columns in a dataset. Or if we wanted to take the score from each of the rows, we can select whichever columns with , separator.
 */
		Dataset<Row> results1 = spark.sql("select score, year from my_students_table where subject = 'French' "); //ultimately we are sitting on an RDD.
		

//		spark.close(); //close method on spark session unsure why not compiling
	
	}
}
