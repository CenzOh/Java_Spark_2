//Lesson 34
package com.virtualpairprogrammers;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.Optional;

import scala.Tuple2;

public class Lesson34 {
	
	public static void main(String[] args) {

	
		//Lesson 34 Left Outer Joins and Optionals
/* Inner joins are def the easiest types of joins to work with. Often the join we need. Can imagine we have a big data file. Maybe its dirty data (invalid values) such as userid 10 could be
 * long since deleted and we dont want to deal with it. Inner join will discard that value from resulting RDD is quite useful. Also user id 1, john has never visited the site so we dont want
 * that data and inner join will exclude that too. If instructor recalls, working with database tables most common type of join is actually an outer join because when doing DB tables we are
 * doing some kind of report. On that report we want to see that john exists but never visited. Of course, depends on requirement and we may find our first choice of join would be the inner join
 * 
 * Lets look at a (left) outer join. They are more complicated to work with as we will see. Lets explain how it works: The left is talking about which direction we are going
 * when we do the join since it is directional. We will have one RDD in our case this is teh elft RDD (user ID and visits) and then we have the right RDD (user id and name). 
 * With a left outer join we start with the left RDD and ensure all the suer ids are present in the resulting join. So we will see three values in our new rdd. FOr IDs 4 and 6
 * we will see same results as before. But for ID 10, it WILL be in resulting RDD. But what will the value be since it has no name? Lets see.
 * 
 */
	
		System.setProperty("hadoop.home.dir", "c:/hadoop"); 
		Logger.getLogger("org.apache").setLevel(Level.WARN); 
		
		SparkConf conf = new SparkConf().setAppName("startingSpark").setMaster("local[*]"); 
		JavaSparkContext sc = new JavaSparkContext(conf);
		
		List<Tuple2<Integer, Integer>> visitsRaw = new ArrayList<>();
		visitsRaw.add(new Tuple2<>(4, 18));
		visitsRaw.add(new Tuple2<>(6, 4));
		visitsRaw.add(new Tuple2<>(10, 9));
		
		List<Tuple2<Integer, String>> usersRaw = new ArrayList<>();
		usersRaw.add(new Tuple2<> (1, "John"));
		usersRaw.add(new Tuple2<> (2, "Bob"));
		usersRaw.add(new Tuple2<> (3, "Alan"));
		usersRaw.add(new Tuple2<> (4, "Doris"));
		usersRaw.add(new Tuple2<> (5, "Marybelle"));
		usersRaw.add(new Tuple2<> (6, "Raquel"));
		
		JavaPairRDD<Integer, Integer> visits = sc.parallelizePairs(visitsRaw);
		JavaPairRDD<Integer, String> users = sc.parallelizePairs(usersRaw);
		
		JavaPairRDD<Integer, Tuple2<Integer, String>> joinedRdd = visits.join(users);
/* no inner join method its just called join(). THis generates a new RDD remember. THink of what parameters will be in the javapairRDD.
 * I though, <Integer, Tuple2<Integer, String>> I was right. Lazy way is when you write joinedRDD select create local variable option and compiler will do it for us.
 *  May get strange ordering because this is running multi threaded
 *  OUTPUT:
 *  (6,(4,Raquel)
 *  (4,(18,Doris)
 */
	
		sc.close();
	
	}
	
}
