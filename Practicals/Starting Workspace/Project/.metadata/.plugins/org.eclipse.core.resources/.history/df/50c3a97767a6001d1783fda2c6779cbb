package com.virtualpairprogrammers;

import java.util.ArrayList;
import java.util.List;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SparkSession;
import static org.apache.spark.sql.functions.*; //now we dont have to explicitly refer to .functions()
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.Metadata;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;


public class Lesson71 {
	
/* Lesson 71 Coding the pivot table in Spark
 * Pivot table on two separate cols is a more compact way to display the same results. WE will have five logging levels with 12 months. 5x12. 16 cells. Rather than 60 rows previously
 * We cant pivot a table with SQL syntax. The SQL language syntax comes from Apache Hive, which Spark is built on top of. According to their documentation, there are no reference to pivots
 * as of the recording. We can still do part of the work with Spark SQL syntax. We can do select for intitial data, then when we want pivot, jump into java api. For now lets stick with java
 * api. 
 */
	
	@SuppressWarnings("resource") 
	public static void main(String[] args) {
		System.setProperty("hadoop.home.dir", "c:/hadoop");
		Logger.getLogger("org.apache").setLevel(Level.WARN);
	
		SparkSession spark = SparkSession.builder().appName("testingSql").master("local[*]")
				.config("spark.sql.warehouse.dir", "file:///c:/tmp/")
				.getOrCreate();
				
		Dataset<Row> big_dataset = spark.read().option("header", true).csv("Src/main/resources/bigLog.txt");
		
		big_dataset.createOrReplaceTempView("logging_table");	
		
// SQL reference
//		Dataset<Row> results = spark.sql
//				("select level, date_format(datetime, 'MMMM') as month, count(1) as total "
//						+ "from logging_table group by level, month order by cast(first(date_format(datetime, 'M')) as int), level"); 
		
// Java API / DataFrame version
		
		big_dataset = big_dataset.select(col("level"), 
				date_format(col("datetime"), "MMMM").alias("month"), 
				date_format(col("datetime"), "M").alias("monthnum").cast(DataTypes.IntegerType) );
		
		big_dataset //no pivot option.
		
		
//		bad_big_dataset2 = big_dataset.groupBy(col("level"), col("month"), col("monthnum")).count();
//
//		bad_big_dataset2 = big_dataset.orderBy(col("monthnum") );
//	
//		
//		bad_big_dataset2 = pivoted_big_dataset.drop(col("monthnum")); 
//		
//		bad_big_dataset2 = big_dataset.orderBy(col("monthnum"), col("level") ); //handled in pivoting
		
		
		big_dataset.show(100); 
		
		spark.close();
	
	}
}
