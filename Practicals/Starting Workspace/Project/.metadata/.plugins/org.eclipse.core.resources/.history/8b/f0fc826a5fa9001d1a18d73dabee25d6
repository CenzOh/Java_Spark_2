package com.virtualpairprogrammers;

import java.util.Arrays;
import java.util.List;
import java.util.Scanner;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import static org.apache.spark.sql.functions.*;


public class Lesson79 {
	
/* Lesson 79 Setting spark.sql.shuffle.partitions
 * We can set a property called spark.sql.shuffle.partitions. It may not affect us in real life since it is again, the issue with small jobs in spark sql. When we go to spark structured
 * streaming section we will work with smaller spark sql jobs since we are working in small batches. One of the default configs works well with big jobs but not for smaller
 * jobs. May not affect us just yet, worth looking at though. Lets look at the spark UI again. NOTE - When doing a grouping, it will do a shuffle, so it creates some partitions of data.
 * Recall we have seen partitions earlier in the course. For each partition there will be a task assigned to the partition. 
 */
	
	@SuppressWarnings("resource") 
	public static void main(String[] args) {
		System.setProperty("hadoop.home.dir", "c:/hadoop");
		Logger.getLogger("org.apache").setLevel(Level.WARN);
	
		SparkSession spark = SparkSession.builder().appName("testingSql").master("local[*]")
				.config("spark.sql.warehouse.dir", "file:///c:/tmp/")
				.getOrCreate();
				
		Dataset<Row> dataset = spark.read().option("header", true).csv("Src/main/resources/biglog.txt");
		
		
// SQL reference
		dataset.createOrReplaceTempView("logging_table"); 

		Dataset<Row> results = spark.sql
			("select level, date_format(datetime, 'MMMM') as month, count(1) as total "
			+ "from logging_table group by level, month order by cast(first(date_format(datetime, 'M')) as int), level"); 
		
		
		
		
		results.show(100);
			
// Java API / DataFrame version		
//		dataset = dataset.select(col("level"), 
//						  date_format(col("datetime"), "MMMM").alias("month"), 
//					  	  date_format(col("datetime"), "M").alias("monthnum").cast(DataTypes.IntegerType) );
//	
//		
//		dataset = dataset.groupBy("level", "month", "monthnum").count().as("total").orderBy("monthnum");
//		dataset = dataset.drop("monthnum");
// 
//		dataset.show(100);

		
		Scanner scanner = new Scanner(System.in); 
		scanner.hasNextLine();
		

		spark.close();
	
	}
}
