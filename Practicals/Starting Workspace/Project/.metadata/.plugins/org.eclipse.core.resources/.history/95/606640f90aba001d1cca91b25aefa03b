package com.virtualpairprogrammers;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.streaming.OutputMode;
import org.apache.spark.sql.streaming.StreamingQuery;
import org.apache.spark.sql.streaming.StreamingQueryException;

public class Lesson142 {

/* Lesson 142  Kafka Structured Streaming Pipelines 
 * 
 * We've been capturing events in kafka, connect multi subscribers to a kafka topic. We pretty much have been making an infinite loop with a delay. Produce new report and print to console
 * DStream prints entire report every second. What we saw with structured version is two things come together. 
 * 1, no batch size in ehre anymore. Works with micro batches. Fancy way to say it works as fast as it can. As soon as it has data, it runs a job
 * 2, sophisticated sink to write to. the streaming query line says it all. .writeStream() this one. Rather than dump to console, we make a stream for the results as well as
 * a stream for incoming data. Update mode, we have powerful results. THis is one example architecture we can build with kafka and spark structured streaming.  
 * 
 * Writ results to file good but better, we can write results to another kafka topic. Different topic called results. We want to write to that topic the results
 * from a certain micro batch. WIll do mainly with the update mode. We want to know, what changed? Also point of second kafka topic? We can put subscriebrs on the topic. 
 */
	
	@SuppressWarnings("resource") 
	public static void main(String[] args) throws StreamingQueryException {
		System.setProperty("hadoop.home.dir", "c:/hadoop"); 
		Logger.getLogger("org.apache").setLevel(Level.WARN); 
		Logger.getLogger("org.apache.spark.storage").setLevel(Level.ERROR); 
		
//boilerplate ^^^^^^
		SparkSession session = SparkSession.builder()
				.master("local[*]")
				.appName("structuredViewingReport")
				.getOrCreate();
		
		session.conf().set("spark.sql.shuffle.partitions", "10"); //small amount will always perform better
		
		Dataset<Row> df = session.readStream() 
				.format("kafka")
				.option("kafka.bootstrap.servers", "localhost:9092")
				.option("subscribe",  "viewrecords")
				.option("kafka.value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
				.load(); 
		
//start dataframe ops
		df.createOrReplaceTempView("viewing_figures");
		
//key, value, timestamp
		Dataset<Row> results = session.sql("select window, cast (value as string) as course_name, sum(5) as seconds_watched from viewing_figures group by window(timestamp, '1 minute') course_name"); 
		
		StreamingQuery query = results 
			.writeStream()
			.format("console")
			.outputMode(OutputMode.Update()) 
			.option("truncate", false)
			.option("numRows", 50)
			.start(); 
		
		query.awaitTermination();
}
