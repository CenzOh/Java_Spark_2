package com.virtualpairprogrammers;

import java.util.ArrayList;
import java.util.List;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.Metadata;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;


public class Lesson66 {
	
/* Lesson 66 Multiple Groupings
 * Lets go further with groupings. Let us group on multi columns and then use a realistic big data set. Now we have a data set with a boiled down form of log messages.
 * All we have is the level and month of message. WE have small set of data but it is true we will have duplicate values like WARN December. THis means we had a warning log message
 * on some day in December of some year. ANd on another occasion, we had another warning message ALSO in december. It could have been the same day as first message. Maybe a different day
 * or even a different year.
 * 
 * Our requirement is to just work out how many messages of each level we were getting each month. So what we have to do is count up duplicates
 */
	
	@SuppressWarnings("resource") 
	public static void main(String[] args) {
		System.setProperty("hadoop.home.dir", "c:/hadoop");
		Logger.getLogger("org.apache").setLevel(Level.WARN);
	
		SparkSession spark = SparkSession.builder().appName("testingSql").master("local[*]")
				.config("spark.sql.warehouse.dir", "file:///c:/tmp/")
				.getOrCreate();
				
		List<Row> inMemory = new ArrayList<Row>(); //import list and arrayList from java.util

		inMemory.add(RowFactory.create("WARN", "2016-12-31 04:19:32") );
		inMemory.add(RowFactory.create("FATAL", "2016-12-31 03:22:34") );
		inMemory.add(RowFactory.create("WARN", "2016-12-31 03:21:21") );
		inMemory.add(RowFactory.create("INFO", "2015-4-21 14:32:21") );
		inMemory.add(RowFactory.create("FATAL", "2015-4-21 19:23:20") );
		
		StructField[] fields = new StructField[] {
				new StructField("level", DataTypes.StringType, false, Metadata.empty() ),
				new StructField("datetime", DataTypes.StringType, false, Metadata.empty())
		};
		
		StructType schema = new StructType(fields);
		Dataset<Row> dataset = spark.createDataFrame(inMemory,  schema); 
		
		dataset.createOrReplaceTempView("logging_table"); 
		
		
		Dataset<Row> results = spark.sql("select level, date_format(datetime, 'MMMM') as month from logging_table"); //adding alias to second column
		
		
		results.show(); 
		
		spark.close();
	
	}
}
