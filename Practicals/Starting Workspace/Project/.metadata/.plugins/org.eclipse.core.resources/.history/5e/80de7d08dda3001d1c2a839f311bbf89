package com.virtualpairprogrammers;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.sql.Column;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.functions;

import static org.apache.spark.sql.functions.*; //use this instead for .col()

public class Lesson62 {
	
/* Lesson 62 Using a Spark Temporary View for SQL
 * We will be looking at FULL SQL syntax in spark SQL. We can use a java API (will look at later). The SQL API is better if we want to focus on the data science rather than coding.
 * We looked at writing filters, there are several approaches to writing filters. Two java-ish ways, either domain specific language (pretty readable but complicated) or lambdas
 * (not as readable). More common way is to use SQL syntax. WE will look at it in more detail. 
 */
	
	@SuppressWarnings("resource") 
	public static void main(String[] args) {
		System.setProperty("hadoop.home.dir", "c:/hadoop");
		Logger.getLogger("org.apache").setLevel(Level.WARN);
	
		SparkSession spark = SparkSession.builder().appName("testingSql").master("local[*]")
				.config("spark.sql.warehouse.dir", "file:///c:/tmp/")
				.getOrCreate();
		
		Dataset<Row> dataset = spark.read().option("header",  true).csv("src/main/resources/exams/students.csv");

// sql
//		Dataset<Row> modernArtResults2 = dataset.filter("subject = 'Modern ARt' AND year >= 2007");
		
// lambda
//		Dataset<Row> modernArtResults = dataset.filter( row -> row.getAs("subject").equals("Modern Art")
//															&& Integer.parseInt(row.getAs("year")) >= 2007);

// functions class
//		Dataset<Row> modernArtResults1 = dataset.filter(col("subject").equalTo( "Modern Art" )
//				  													  .and(col("year").geq(2007)) );
		
		

		

//		spark.close(); //close method on spark session unsure why not compiling
	
	}
}
