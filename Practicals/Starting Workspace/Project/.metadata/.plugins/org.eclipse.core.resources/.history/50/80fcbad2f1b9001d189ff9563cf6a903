package com.virtualpairprogrammers;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.SparkConf;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;

import scala.Tuple2;

public class Lesson126 {

/* Lesson 126 Streaming Aggregation
 * 
 * Last time we got the streaming up and running. We have flakey sockets not prod standard so lets move onto Kafka based implementation which is more prod standard. 
 * But we proved the principle of spark streaming. Lets also do aggregation on the batch. Terminate both and lets add some things.
 * Lets count how many inofs, WARNs, DEBUGs. Log level is separated from timestamp with a comma. Lets actually remove the time stamp
 */
	
	@SuppressWarnings("resource") 
	public static void main(String[] args) throws InterruptedException {
		System.setProperty("hadoop.home.dir", "c:/hadoop"); 
		Logger.getLogger("org.apache").setLevel(Level.WARN); 
		Logger.getLogger("org.apache.spark.storage").setLevel(Level.ERROR); 
	
		SparkConf conf = new SparkConf().setMaster("local[*]").setAppName("startingSpark");

		JavaStreamingContext sc = new JavaStreamingContext(conf, Durations.seconds(2)); //changing batch size to be quicker

		JavaReceiverInputDStream<String> inputData = sc.socketTextStream("localhost", 8989);

		JavaDStream<String> results = inputData.map(item -> item); //updated method to be a string so we can have string method
		results.print(); 

//		results = results.map(rawLogMessage -> rawLogMessage.split(",")[0]); //split on commas, extract first element which is log level
		JavaPairDStream<String, Long> pairDStream = results.mapToPair(rawLogMessage -> new Tuple2<> (rawLogMessage.split(",")[0], 1L)); //import java pair D Stream
// we use mapToPair() this time, use tuple2 of string for first param with log message, and long for second param which counts num of occurances
		
		pairDStream = pairDStream.reduceByKey((x,y) -> x + y); //reduce by key to add up our total count of ERRORs, WARNs, etc.
		
		pairDStream.print();
/* -------------------------------------------
Time: 1677868284000 ms
-------------------------------------------
(DEBUG,746)
(WARN,135)
(ERROR,15)
(INFO,569)
 * 
 * Great it works! For each batch we get around 1k messages. We get a ton of debug messages. Sometimes we would see a FATAL message. These totals are the totals for the LAST batch.
 * Not getting historic data. What if we want updates every second and aggregation of last hours worth of data> We will do that shortly.
 */
		
		

		sc.start();
		sc.awaitTermination();
		
/*  Lets start both up and lets see what ouptuts
 * -------------------------------------------
Time: 1677862710000 ms
-------------------------------------------
DEBUG
WARN
WARN
DEBUG
DEBUG
DEBUG
DEBUG
DEBUG
DEBUG
WARN
...

 * Okay now we need a count
 */
		
	}
}