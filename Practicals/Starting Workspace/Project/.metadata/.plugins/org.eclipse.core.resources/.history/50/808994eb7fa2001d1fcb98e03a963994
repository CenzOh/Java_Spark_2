package com.virtualpairprogrammers;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class Lesson58 {
	
/* Lesson 58 Dataset Basics
 * In this chapter we will look at the dataset API in Spark SQL. We will look at filters first. We already did config work to make spark point at a datasource. Returns as a dataset.
 * That is the core object in SQL. show() method shows first twenty rows of the data. As we mentioned previously count is implemented as an operation against RDD under the hood.
 * So it will be the kind of maps and reduces that we have seen in the previous section.
 * 
 * Now that we are working in Spark SQL we can work in higher levels of abstraction. All of the oeprations that we can perform will be implemented as big data operations, maps, reducers,
 * filters and so on. We will work in the higher level API.
 *
 */
	
	@SuppressWarnings("resource") //boiler plate start
	public static void main(String[] args) {
		System.setProperty("hadoop.home.dir", "c:/hadoop");
		Logger.getLogger("org.apache").setLevel(Level.WARN);
	
		SparkSession spark = SparkSession.builder().appName("testingSql").master("local[*]")
				.config("spark.sql.warehouse.dir", "file:///c:/tmp/")
				.getOrCreate();
		
		Dataset<Row> dataset = spark.read().option("header",  true).csv("src/main/resources/exams/students.csv");


		dataset.show();

		long numberOfRows = dataset.count(); 
		System.out.println("There are " + numberOfRows + " records");
		
		
//		spark.close(); //close method on spark session unsure why not compiling
	
	}
}
