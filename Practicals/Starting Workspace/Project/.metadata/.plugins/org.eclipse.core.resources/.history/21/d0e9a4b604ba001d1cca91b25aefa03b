package com.virtualpairprogrammers;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class Lesson137 {

/* Lesson 137 Structured Streaming Overview
 * 
 * We experienced spark structured streaming with abstraction of DStream. lets us use RDDs. API with weird RDD like flavor. Under teh hood spark UI uses RDDs built for each batch
 * More recently new api introduced called structured streaming. This gives us access to the spark sql model. We can use dataframes, datasets, or sql api whatever we like.
 * 
 * Spark docs have a separate section for structured streaming. Again obv difference between structured stream and DStreams are the APIs. However it look slike dev defforts are
 * directed to structured streaming. 
 * 
 * Devs are working on continuous processing feature. Low latencies, end to end processing. Not batch size. Trying to get batch sizes down as small as possible. WHen we program this,
 * we dont have easy access to batch size but they are hidden away from us. CLear indication they want us to forget about the batch size and think about the continuous streams of data. 
 * 
 * Guide in the Structured Streaming Programming guide. Work with spark session obj instead of spark conf. Make a new class called ViewingFiguresStructuredVersion
 * Recall last video we updated pom.xml to have version 2.4.0 make sure to update that. We dont have a dependency on spark structured streaming
 * 
 * We need a dependency called spark-sql-kafka-0-10_2.11 We dont need extra dependency for structured streaming, we have that through spark streaming artifact. 
 */
	@SuppressWarnings("resource") 
	public static void main(String[] args) throws InterruptedException {
		System.setProperty("hadoop.home.dir", "c:/hadoop"); 
		Logger.getLogger("org.apache").setLevel(Level.WARN); 
		Logger.getLogger("org.apache.spark.storage").setLevel(Level.ERROR); 
		
//boilerplate ^^^^^^
		SparkSession session = SparkSession.builder()
				.master("local[*]")
				.appName("structuredViewingReport")
				.getOrCreate();
		
		Dataset<Row> df = session.readStream() //compiler error because we need to set options on this string. Havent mentioned kafka yet. call option method to set key value pairs
				.format("kafka")
				.option("kafka.bootstrap.servers", "localhost:9092")
				.option("subscribe",  "viewrecords")
				.load(); //final line to make this work.
		
//start dataframe ops
	
	}
}