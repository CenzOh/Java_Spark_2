package com.virtualpairprogrammers;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.ml.feature.VectorAssembler;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class Lesson97 {

/* Lesson 97, splitting training data with random splits
 * 
 * This is a big data set, has about 21k rows. Its big enough to split down into two smaller data sets. One to train, one to test. How many records we choose to put in each dataset is up to us
 * but general rule is 80% trainings, 20% testing, to do this, datasets have a method called .randomSplit(). It takes in an array of doubles and we specify the percentage in each data set.
 * This method returns an array of datasets, one for training and one for testing.
 */
		
	public static void main(String[] args) {
		
		System.setProperty("hadoop.home.dir", "c:/hadooop"); 
		Logger.getLogger("org.apache").setLevel(Level.WARN);
		
		SparkSession spark = SparkSession.builder()
				.appName("Gym Competitors")
				.config("spark.sql.warehouse.dir", "file:///c:/tmp/")
				.master("local[*]")
				.getOrCreate();
		
		Dataset<Row> csvData = spark.read()
				.option("header",  true)
				.option("inferSchema", true)
				.csv("src/main/resources/kc_house_data.csv"); 

		VectorAssembler vectorAssembler = new VectorAssembler()
				.setInputCols(new String[] {"bedrooms", "bathrooms", "sqft_living"} )
				.setOutputCol("features");
		
//now lets take our CSV data and transform it with our vector assembler. We will set the results to be a data set of rows and we'll call this one model input data.
//gives us csvData with extra col called features. Lets drop all cols except price and features
		Dataset<Row> modelInputData = vectorAssembler.transform(csvData)
				.select("price", "features")
				.withColumnRenamed("price",  "label");
		
		modelInputData.show();
		
	}
}