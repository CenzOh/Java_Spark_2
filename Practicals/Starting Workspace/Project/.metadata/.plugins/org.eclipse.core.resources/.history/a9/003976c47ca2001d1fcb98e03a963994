package com.virtualpairprogrammers;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;

public class Lesson57 {
	
/* Lesson 57 SparkSQL getting started
 * We have been previously working on various experiments in Main.java. For this section we will modify the code in that section (I will still put code in different java files)
 * for easier readability. There is another starting workspace with up to date code in case we skipped the first section and jumped straight into this section. 
 * In section 1, one of the joys of working with spark is that there are not a lot of dependencies that we need / no need for specific software. We simply have a maven pom.xml file
 * and we can see in there the dependencies are basic, spark core, spark SQL (we added previously for this part of the course), and apache hadoop (access HDFS file system).
 * 
 * In case you skipped first section, what we have to do is run the pom.xml as a maven build, goal is `eclipse:eclipse` so eclipse can download all the jar files required for spark.
 * The previous versions we were given was version 2.0 which is an old version of Spark. As of this video, you can check the apache spark page and see there are a lot of current versions
 * latest during the recording is spark 2.3.2. As of when I am writing this we can use Spark 3. Would make sense to upgrade these versions, you can easily do that in the pom.xml file 
 * just change <version>2.3.2</version> under spark core dependency and sql dependency. HOWEVER, issue of doing our latest version is we may have variances between
 * what happens in our development enviornments and what happens on video. Recommended to stick to 2.3.2 since recording has that.
 * 
 * The complexity of this is that the artifact IDs for both core and sql there is a version num on the artifact id. It has .10. On spark download page, we can see that `as of spark 2.0,
 * Spark is built with Scala 2.11 by default` thats what we are seeing here, the version of Scala. For spark 2.3.2, the artifact needs to be 2.11 for Scala version. Make sure to update that too 
 * under <artifactId>spark-core_2.11</artifact> Run the pom.xml again. Lots of download should occur. 
 * 
 * Now take note, may have happened to us, this was complaining that some jar files could not be written properly. To check if everythings okay, refresh project. Instructor received this
 * red X on the project. Look in our main or other files which should compile correctly. We may see there are all errors on the Spark objects. This could have been a corruption of
 * our Maven repo. Happens frequently when upgrading a library version. Easy way to fix this: to go home directory, find hidden folder called .m2 this is the maven cache. Delete this directory,
 * run the build again so it forces to download everything again. May take longer but it works.
 * 
 * Now we can go to our main class this is the section where we were experimenting with RDDs, now we will find how to convert this Spark SQL program. Need to work with structured data section. 
 * We can find this under the exams subfolder in resources. There is a CSV file inside called students.csv. Open it, may take a minute. Pretty big file about 60 MB. In our code we can pretty much 
 * start fresh so I'll write what we would have to do below:
 */
	
	@SuppressWarnings("resource") //boiler plate start
	public static void main(String[] args) {
		System.setProperty("hadoop.home.dir", "c:/hadoop");
		Logger.getLogger("org.apache").setLevel(Level.WARN);
	
//		SparkConf conf = new SparkCOnf().setAppName("startingSpark").setMaster("local[*]");
//		JavaSparkContext sc = new JavaSparkContext(conf);

/* boilerplate end, pretty much all we need to begin! SparkConf and Spark Context lines we have seen with RDDs. All the work we did in RDD library was done in Spark Context. We will NOT use
 * them again but we can compare what we need to do to work in Spark SQL. Similar recipe. Just that end result will end up with SparkSession. This is how we will operate SQL.
 */
	}
}
