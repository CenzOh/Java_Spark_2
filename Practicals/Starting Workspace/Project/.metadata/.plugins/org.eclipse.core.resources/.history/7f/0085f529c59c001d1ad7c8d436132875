//Lesson 28
package com.virtualpairprogrammers;

import java.util.Arrays;
import java.util.List;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

import scala.Tuple2;

public class Lesson28 {
	
	public static void main(String[] args) {

	
		//Lesson 28-32 AWS EMR optional section, will document some steps
/* Lesson 28 How to start EMR Spark Cluster
 * We will be running our Spark app on a real hardware cluster using Amazon EMR. Will be documenting how to do this. Assuming account for AWS already created and familiarity with AWS.
 * Even if you dont have an account, this will be good for insight. Everything done in the chapter will incur cost. If failed to delete or terminate resources you will be billed!
 * Objective - deploy keyword count app to live hardware. Basically we need cluster manager. Spark supports hadoop cluster manager. Hadoop is NOT just map reduce. It is collection
 * of tools including map reduce but also HDFS file system and cluster manager. If setting up cluster by hand, we can set up a cluster in the same way that we would set up a hadoop cluster.
 * Can also use existing AWS service called EMR. EMR stands for Elastic Map Reduce, this is amazons implementation of hadoop in the cloud.
 * 
 */
	
		System.setProperty("hadoop.home.dir", "c:/hadoop"); 
		Logger.getLogger("org.apache").setLevel(Level.WARN); 
		
		SparkConf conf = new SparkConf().setAppName("startingSpark").setMaster("local[*]"); 
		JavaSparkContext sc = new JavaSparkContext(conf);
	
	
		sc.close();
	
	}

}
