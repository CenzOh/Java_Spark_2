//Lesson 50
package com.virtualpairprogrammers;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Scanner;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.Optional;

import scala.Tuple2;

public class Lesson50 {
	
	public static void main(String[] args) {

	
		//Lesson 50 Narrow vs Wide Transformations
/* Lets run through simple example we will code up as well to show resulting execution plan over on real spark. Imagine we have a text file, this is terabytes from some server and
 * is logging like we saw before. We have logging level, followed by a colon, followed by a date and time. Ex:
 * WARN: Tuesday 4 September 0405
 * On the execution plan, we will call the text file method which adds a node into the execution plan. What it really does is tell the worker node to load fragments of text into memory
 * and we are going to end up with a series of partitions (introduced in opening chapter). As a reminder, a partition is a chunk of data. Now imagine on our deployment, we are going to have
 * worker nodes like when we ran elastic map reduce while thinking about it for a tera byte file. We need MORE than two worker nodes for this since we need to fit each
 * partition into ram. Assume that two worker nodes are sufficient for our example. Note that we will typically see multiple partitions on each of the nodes.
 * 
 */
	
		System.setProperty("hadoop.home.dir", "c:/hadoop"); 
		Logger.getLogger("org.apache").setLevel(Level.WARN); 
		
		SparkConf conf = new SparkConf().setAppName("startingSpark").setMaster("local[*]"); 
		JavaSparkContext sc = new JavaSparkContext(conf);
		
		//all from lesson 26
		JavaRDD<String> initialRdd = sc.textFile("src/main/resources/subtitles/input.txt");	
		
		JavaRDD<String> lettersOnlyRdd = initialRdd.map( sentence -> sentence.replaceAll(" ^[a-zA-Z\\s] ", "").toLowerCase()); 
		
		JavaRDD<String> removedBlankLines = lettersOnlyRdd.filter( sentence -> sentence.trim().length() > 0); 
	
		JavaRDD<String> justWords = removedBlankLines.flatMap( sentence -> Arrays.asList(sentence.split(" ")).iterator());
		
		JavaRDD<String> blankWordsRemoved = justWords.filter(word -> word.trim().length() > 0);
	
		JavaRDD<String> justInterestingWords = blankWordsRemoved.filter( word -> Util.isNotBoring(word));
		
		JavaPairRDD<String, Long> pairRdd = justInterestingWords.mapToPair( word -> new Tuple2<String, Long>(word, 1L));
	
		JavaPairRDD<String, Long> totals = pairRdd.reduceByKey((value1, value2) -> value1 + value2);

		JavaPairRDD<Long, String> switched = totals.mapToPair(tuple -> new Tuple2<Long, String> (tuple._2, tuple._1 ));

		JavaPairRDD<Long, String> sorted = switched.sortByKey(false);
		
		Scanner scanner = new Scanner(System.in); //scanner obj from java.util this allows us to read from the console
		scanner.nextLine(); //this will make the console wait for a text input so we can visit the web ui
		

		sc.close();
	
	}
	
}
